\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Using RRTs to Navigate Gibson Environments}

\author{Paul McKerley\\
George Mason University\\
{\tt\small pmckerle@masonlive.gmu.edu}
}

\frenchspacing
\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  Produce efficient maps to navigate two-dimensional floorplans
  extracted from three-dimensional Gibson Environment buildings.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

  The goal of this project was to produce road maps for Gibson
  Environment buildings that would allow robots to navigate from one
  arbitrary point in the map to another. This navigation should find
  reasonable straight-line paths. My approach was to use
  Rapidly-Exploring Random Trees, which provided maps in a matter of
  seconds. A* search can traverse these maps, and the results paths
  refined to be straighter. Many samples paths can be found and
  refined, with the resulting paths added to the map. These refined
  paths give highly efficient paths between rooms.

\section{Approach}

There are several steps requirement to produce a roadmap of a Gibson
Environment structure.

\subsection{Extract Two-Dimensional Floorplan}

The Gibson buildings come as three-dimensional graphs. To extract
two-dimensional floor plans I used a Python module called meshcut.py
(kindly provided by Yimeng Li.) \ref{fig:Allensville_3D} shows a 3D
representation of the Allensville apartment. When a 2D images is extracted
at a height of 0.5 meters the result is found in \ref{fig:Allensville_2D}.
This picture is an image saved from the Python module matplotlib. But it
can be converted to an OpenCV-compatible numpy array and mantipulated in
memory or saved to file.

\begin{centering}
\begin{figure}[ht]
\caption{3D Image of Allensville}
\centering
\includegraphics[width=5cm]{Allensville_thumb.png}
\label{fig:Allensville_3D}
\end{figure}
\begin{figure}[ht]
\caption{2D Image of Allensville at 0.5 m}
\centering
\includegraphics[width=5cm]{Allensville.png}
\label{fig:Allensville_2D}
\end{figure}
\end{centering}

\subsection{Preparing Floorplan for Geometry Checks}

The size of the buildings are not large compared to the expected size
of robot we would expect to operate in them. There also need to be
many checks of lines for collisions with solid objects (walls and
other items in the rooms.) An efficient way to do these checks is by
rasterizing the maps and using Bresenham's line-drawing algorithm to
draw on the image. Solid areas are represented as black pixels, and
free areas as white pixels. The way to do this to use opencv.floodFill()
starting at a point in the free space to fill the freespace with white
pixels.

Even though the dataset documentation say most of the Gibson maps have
hard the holes filled, it is still the case that the 2D has visible
holes in the walls. This makes the floodFill() operation fail. These
gaps can be filled ``manually'', but this is tedious. So a function
finds the end-points to all lines, and connects them to the nearest
endpoint of another line. This can be done efficiently with numpy
representations of the line arrays. Not all maps work, but enough to
have a decent set to work with.

Finally, There are still small gaps between objects that are evidently
too small for a robot to fit through, but which the RRT lines would
traverse. To get rid of these, opencv.erode is applied to the
image. This also has the effect of providing some space from the walls
that represents the thickness of a robot. Frankly, I used only a few
pixels of erosion, so the spacing effect is not realistically
large. However, more erosion could certainly be used to get the
correct effect.

\ref{fig:free} shows a rasterized free space map for Allensville after
gaps have been filled and erosion applied.

\begin{centering}
\begin{figure}[ht]
\caption{Allensville free space image.}
\centering
\includegraphics[width=5cm]{free.png}
\label{fig:free}
\end{figure}
\end{centering}

In summary, the algorithm to convert a set of numpy arrays, each
representing a line in the map, to the rasterized free space image,
is:

\begin{enumerate}
\item Find minimum and maximum x and y positions of lines.
\item Pad lines out with configurable quanity (typically 0.5 m in examples.)
\item Draw lines with opencv.polylines() on free image initialized to black.
\item Calculate lines to fill gaps and draw free image.
\item Floodfill image with white starting at fixed point in image (this should be made configurable.)
\item Erode image with a 5x5 mask a configurable number of times (5 by default.)
\end{enumerate}

\subsection{Miscellaneous}


\subsection{References}

List and number all bibliographical references in 9-point Times,
single-spaced, at the end of your paper. When referenced in the text,
enclose the citation number in square brackets, for
example~\cite{Authors06}.  Where appropriate, include the name(s) of
editors of referenced books.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Method & Frobnability \\
\hline\hline
Theirs & Frumpy \\
Yours & Frobbly \\
Ours & Makes one's heart Frob\\
\hline
\end{tabular}
\end{center}
\caption{Results.   Ours is better.}
\end{table}

%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.  Please ensure that any point you wish to
make is resolvable in a printed copy of the paper.  Resize fonts in figures
to match the font in the body text, and choose line widths which render
effectively in print.  Many readers (and reviewers), even of an electronic
copy, will choose to print your paper in order to read it.  You cannot
insist that they do otherwise, and therefore must not assume that they can
zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use
\verb+\includegraphics+, and to specify the  figure width as a multiple of
the line width as in the example below
{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}


%-------------------------------------------------------------------------
\subsection{Color}

Color is valuable, and will be visible to readers of the electronic copy.
However ensure that, when printed on a monochrome printer, no important
information is lost by the conversion to grayscale.

%------------------------------------------------------------------------
\section{Final copy}

You must include your signed IEEE copyright release form when you submit
your finished paper. We MUST have this form before your paper can be
published in the proceedings.

Please direct any questions to the production editor in charge of these
proceedings at the IEEE Computer Society Press: Phone (714) 821-8380, or
Fax (714) 761-1784.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
